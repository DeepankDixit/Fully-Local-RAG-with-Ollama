# Fully-Local-RAG-with-Ollama
RAG based conversational AI both using Ollama mistral and nomic-embed-text embeddings running fully locally

### Installing the Dependencies

1. Clone the repository to your local machine
2. Run `pip install -r requirements.txt` to install the required dependencies

## How to run it locally

1. Run the `fully_local_rag.py` file from the project directly using `streamlit run fully_local_rag.py`
2. This will launch the app in your web browser
3. You can load multiple PDFs and DOCX files and click on Process. 
4. Once the processing is complete, you can chat with the bot about uploaded documents.
